{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramePreprocessor:\n",
    "    def __init__(self, session, resize_width, resize_height):\n",
    "        self.session = session\n",
    "        self.frame = tf.placeholder(shape=(210, 160, 3), dtype=tf.uint8)\n",
    "        self.gray = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.crop = tf.image.crop_to_bounding_box(self.gray, 34, 0, 160, 160)\n",
    "        self.resize = tf.image.resize_images(self.crop, (resize_width, resize_height), \n",
    "                                             method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, frame):\n",
    "        return self.session.run(self.resize, {self.frame: frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_width, input_height, num_stacked_frames, num_actions, learning_rate):\n",
    "        self.input = tf.placeholder(shape=(None, input_width, input_height, num_stacked_frames), dtype=tf.float32)\n",
    "        self.normalized = self.input / 255\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.normalized, filters=32, kernel_size=(8,8), strides=4, kernel_initializer=tf.initializers.variance_scaling(scale=2), padding='valid', activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=(4,4), strides=2, kernel_initializer=tf.initializers.variance_scaling(scale=2), padding='valid', activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=(3,3), strides=1, kernel_initializer=tf.initializers.variance_scaling(scale=2), padding='valid', activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "        self.conv4 = tf.layers.conv2d(inputs=self.conv3, filters=1024, kernel_size=(7,7), strides=1, kernel_initializer=tf.initializers.variance_scaling(scale=2), padding='valid', activation=tf.nn.relu, use_bias=False, name='conv4')\n",
    "        \n",
    "        self.split1, self.split2 = tf.split(self.conv4, 2, 3)\n",
    "        self.split1 = tf.layers.flatten(self.split1)\n",
    "        self.split2 = tf.layers.flatten(self.split2)\n",
    "        self.valuefn = tf.layers.dense(inputs=self.split1, units=1, kernel_initializer=tf.initializers.variance_scaling(scale=2), name='valuefn')\n",
    "        self.advantagefn = tf.layers.dense(inputs=self.split2, units=num_actions, kernel_initializer=tf.initializers.variance_scaling(scale=2), name='advantagefn')\n",
    "        \n",
    "        self.q_actual = self.valuefn + tf.subtract(self.advantagefn, tf.reduce_mean(self.advantagefn, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_actual, axis=1)\n",
    "        \n",
    "        self.q_target = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=(None), dtype=tf.int32)\n",
    "        self.q_pred = tf.reduce_sum(tf.multiply(self.q_actual, tf.one_hot(self.action, num_actions, dtype=tf.float32)), axis=1)\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.q_target, predictions=self.q_pred))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Breakout:\n",
    "    def __init__(self, session, frame_width, frame_height, num_stacked_frames):\n",
    "        self.env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "        self.fp = FramePreprocessor(session, frame_width, frame_height)\n",
    "        self.lives = self.env.ale.lives()\n",
    "        self.state = None\n",
    "        self.num_stacked_frames = num_stacked_frames\n",
    "    \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        self.lives = self.env.ale.lives()\n",
    "        self.state = np.repeat(self.fp.process(frame), self.num_stacked_frames, axis=2)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        frame, reward, done, _ = self.env.step(action)\n",
    "        life_lost = done or (self.env.ale.lives() < self.lives)\n",
    "        self.lives = self.env.ale.lives()\n",
    "        self.state = np.append(self.state[:, :, 1:], self.fp.process(frame), axis=2)\n",
    "        return self.state, reward, done, life_lost\n",
    "    \n",
    "    def close():\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionSelector:\n",
    "    def __init__(self, session, dqn, num_actions, epsilon_decay_intervals, epsilon_values_at_intervals):\n",
    "        self.session = session\n",
    "        self.dqn = dqn\n",
    "        self.num_actions = num_actions\n",
    "        self.edi = epsilon_decay_intervals\n",
    "        self.evai = epsilon_values_at_intervals\n",
    "    \n",
    "    def epsilon(self, frame_number):\n",
    "        i = len(self.edi) - 1\n",
    "        for j in range(len(self.edi) - 1):\n",
    "            if self.edi[j] <= frame_number < self.edi[j+1]:\n",
    "                i = j\n",
    "                break\n",
    "        m = (self.evai[i] - self.evai[i+1]) / (self.edi[i] - self.edi[i+1])\n",
    "        c = self.evai[i] - m * self.edi[i]\n",
    "        return m * frame_number + c\n",
    "    \n",
    "    def epsilon_greedy_action(self, frame_number, state):\n",
    "        if np.random.uniform() < self.epsilon(frame_number):\n",
    "            return np.random.randint(0, self.num_actions)\n",
    "        return greedy_action(state)\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        return self.session.run(self.dqn.best_action, {self.dqn.input: [state]})[0]\n",
    "    \n",
    "    def plot_epsilon(self):\n",
    "        plt.plot([self.epsilon(i) for i in range(0, self.edi[-1], 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, batch_size, frame_width, frame_height, num_stacked_frames):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.num_stacked_frames = num_stacked_frames\n",
    "        self.frames = np.empty((self.capacity, frame_width, frame_height), dtype=np.uint8)\n",
    "        self.actions = np.empty(self.capacity, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.capacity, dtype=np.float32)\n",
    "        self.dones = np.empty(self.capacity, dtype=np.bool)\n",
    "        self.current = 0\n",
    "        self.count = 0\n",
    "        self.indices = np.empty(batch_size, dtype=np.int32)\n",
    "        self.prestates = np.empty((batch_size, num_stacked_frames, frame_width, frame_height), dtype=np.uint8)\n",
    "        self.poststates = np.empty((batch_size, num_stacked_frames, frame_width, frame_height), dtype=np.uint8)\n",
    "    \n",
    "    def add(self, frame, action, reward, done):\n",
    "        self.frames[self.current] = frame\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.dones[self.current] = done\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.capacity\n",
    "    \n",
    "    def sample(self):\n",
    "        indices = []\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = np.random.randint(0, self.count - (self.num_stacked_frames + 1))\n",
    "                if self.dones[index : index + self.num_stacked_frames].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index + self.num_stacked_frames - 1\n",
    "            self.prestates[i] = self.frames[index : index + self.num_stacked_frames, ...]\n",
    "            self.poststates[i] = self.frames[index + 1 : index + self.num_stacked_frames + 1, ...]\n",
    "        return (np.transpose(self.prestates, axes=(0, 2, 3, 1)), \n",
    "                self.actions[self.indices], \n",
    "                self.rewards[self.indices], \n",
    "                np.transpose(self.poststates, axes=(0, 2, 3, 1)), \n",
    "                self.dones[self.indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedDQNUpdater:\n",
    "    def __init__(self, session, main_dqn_scope, fixed_dqn_scope):\n",
    "        self.session = session\n",
    "        self.main_dqn_vars = tf.trainable_variables(scope=main_dqn_scope)\n",
    "        self.fixed_dqn_vars = tf.trainable_variables(scope=fixed_dqn_scope)\n",
    "            \n",
    "    def update(self):\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            self.session.run(self.fixed_dqn_vars[i].assign(var.value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Q_\\text{target}(s,a) &= r + \\gamma \\; \\textrm{max}\\; Q_\\text{target}(s',a') &\\text{Normal DQN}\\\\\n",
    "Q_\\text{target}(s,a) &= r + \\gamma \\; Q_\\text{target} \\left(s',\\text{argmax} \\; Q_\\text{fixed}(s',a') \\right)&\\text{Double DQN} \\\\\n",
    "Q_\\text{target}(s,a) &= r \\quad\\quad \\text{if} \\; s' \\; \\text{is terminal} &\\text{Double DQN}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    FRAME_WIDTH = 84\n",
    "    FRAME_HEIGHT = 84\n",
    "    NUM_STACKED_FRAMES = 4\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    REPLAY_MEMORY_CAPACITY = 1000000\n",
    "\n",
    "    EPSILON_DECAY_INTERVALS = [0, 50000, 1000000, 30000000]\n",
    "    EPSILON_VALUES_AT_INTERVALS = [1, 1, 0.1, 0.01]\n",
    "\n",
    "    MAIN_DQN_SCOPE = \"main_dqn\"\n",
    "    FIXED_DQN_SCOPE = \"fixed_dqn\"\n",
    "    MAX_TRAIN_FRAMES = 30000000\n",
    "    MAX_EPISODE_LENGTH = 18000\n",
    "    MIN_REPLAY_MEMORY_SIZE = 50000\n",
    "    FIXED_DQN_UPDATE_FREQ = 10000\n",
    "    MAIN_DQN_UPDATE_FREQ = 4\n",
    "\n",
    "    METRICS_OUTPUT_FREQ = 10\n",
    "    METRICS_RUNNING_MEAN_LOOKBACK = 100\n",
    "\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "        self.game = Breakout(session, self.FRAME_WIDTH, self.FRAME_HEIGHT, self.NUM_STACKED_FRAMES)\n",
    "        num_actions = self.game.env.action_space.n\n",
    "        with tf.variable_scope(self.MAIN_DQN_SCOPE):\n",
    "            self.main_dqn = DQN(self.FRAME_WIDTH, self.FRAME_HEIGHT, self.NUM_STACKED_FRAMES, num_actions, self.LEARNING_RATE)\n",
    "        with tf.variable_scope(self.FIXED_DQN_SCOPE):\n",
    "            self.fixed_dqn = DQN(self.FRAME_WIDTH, self.FRAME_HEIGHT, self.NUM_STACKED_FRAMES, num_actions, self.LEARNING_RATE)\n",
    "        self.action_selector = ActionSelector(session, self.main_dqn, num_actions, self.EPSILON_DECAY_INTERVALS, self.EPSILON_VALUES_AT_INTERVALS)\n",
    "        self.memory = ReplayMemory(self.REPLAY_MEMORY_CAPACITY, self.BATCH_SIZE, self.FRAME_WIDTH, self.FRAME_HEIGHT, self.NUM_STACKED_FRAMES)\n",
    "        self.fixed_dqn_updator = FixedDQNUpdater(session, self.MAIN_DQN_SCOPE, self.FIXED_DQN_SCOPE)\n",
    "        self.frame_number = 0\n",
    "        self.episode_number = 0\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self):\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        while self.frame_number < self.MAX_TRAIN_FRAMES:\n",
    "            state = self.game.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.MAX_EPISODE_LENGTH):\n",
    "                action = self.action_selector.epsilon_greedy_action(self.frame_number, state)\n",
    "                next_state, reward, done, life_lost = self.game.step(action)\n",
    "                self.frame_number += 1\n",
    "                episode_reward += reward\n",
    "                self.memory.add(state[:, :, -1], action, reward, life_lost)\n",
    "                state = next_state\n",
    "                if self.frame_number % self.MAIN_DQN_UPDATE_FREQ == 0 and self.frame_number > self.MIN_REPLAY_MEMORY_SIZE:\n",
    "                    self.losses.append(self.run_update())\n",
    "                if self.frame_number % self.FIXED_DQN_UPDATE_FREQ == 0 and self.frame_number > self.MIN_REPLAY_MEMORY_SIZE:\n",
    "                    self.fixed_dqn_updator.update()\n",
    "                if done:\n",
    "                    break\n",
    "            self.rewards.append(episode_reward)\n",
    "            self.episode_number += 1\n",
    "            if self.episode_number % self.METRICS_OUTPUT_FREQ == 0:\n",
    "                print(f\"Episodes run: {self.episode_number} | Frames seen: {self.frame_number} | Rewards running mean: {np.mean(self.rewards[-self.METRICS_RUNNING_MEAN_LOOKBACK:])}\")\n",
    "\n",
    "    def run_update(self):\n",
    "        prestates, actions, rewards, poststates, dones = self.memory.sample()\n",
    "        best_actions_in_poststates = self.session.run(self.main_dqn.best_action, {self.main_dqn.input: poststates})\n",
    "        all_actions_q_values = self.session.run(self.fixed_dqn.q_actual, {self.fixed_dqn.input: poststates})\n",
    "        best_action_q_values = all_actions_q_values[range(self.BATCH_SIZE), best_actions_in_poststates]\n",
    "        q_target = rewards + (self.DISCOUNT_FACTOR * best_action_q_values * (1 - dones))\n",
    "        loss, _ = self.session.run([self.main_dqn.loss, self.main_dqn.update], \n",
    "                                   {self.main_dqn.input: prestates, \n",
    "                                    self.main_dqn.q_target: q_target, \n",
    "                                    self.main_dqn.action: actions})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    trainer = Trainer(session)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
